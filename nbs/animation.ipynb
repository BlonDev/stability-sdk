{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWXrHouW3pq_"
      },
      "source": [
        "# Animation example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jCZ-IphH3prD"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    outputs_path = \"/content/gdrive/MyDrive/AI/StabilityAnimations\"\n",
        "    !mkdir -p $outputs_path\n",
        "except:\n",
        "    outputs_path = \".\"\n",
        "print(f\"Animations will be saved to {outputs_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zj56t6tc3prF"
      },
      "outputs": [],
      "source": [
        "#@title Connect to the Stability API\n",
        "\n",
        "# Grab GRPC protobuf definitions from the animation branch\n",
        "!git clone -b animation https://github.com/Stability-AI/api-interfaces.git\n",
        "\n",
        "import grpc\n",
        "import sys\n",
        "sys.path.append(\"api-interfaces/gooseai/generation\")\n",
        "import generation_pb2 as generation\n",
        "import generation_pb2_grpc as generation_grpc\n",
        "\n",
        "# GRPC endpoint and engines\n",
        "GRPC_HOST = \"\" #@param {type:\"string\"}\n",
        "API_KEY = \"\" #@param {type:\"string\"}\n",
        "GENERATE_ENGINE_ID = 'stable-diffusion-v1-5'\n",
        "INPAINT_ENGINE_ID = 'stable-diffusion-v1-5'\n",
        "TRANSFORM_ENGINE_ID = 'transform-server-v1'\n",
        "\n",
        "def open_channel(host: str, api_key: str = None) -> generation_grpc.GenerationServiceStub:\n",
        "    print(f\"Opening channel {host}\")\n",
        "    if host.endswith(\":443\"):\n",
        "        call_credentials = []\n",
        "        call_credentials.append(grpc.access_token_call_credentials(api_key))\n",
        "        channel_credentials = grpc.composite_channel_credentials(\n",
        "            grpc.ssl_channel_credentials(), *call_credentials\n",
        "        )\n",
        "        channel = grpc.secure_channel(host, channel_credentials)\n",
        "    else:\n",
        "        channel = grpc.insecure_channel(host)\n",
        "    return generation_grpc.GenerationServiceStub(channel)\n",
        "\n",
        "# Connect to Stability API\n",
        "stub = open_channel(GRPC_HOST, api_key=API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3Rf9Fcqi3prF"
      },
      "outputs": [],
      "source": [
        "#@title Code definitions\n",
        "\n",
        "import bisect\n",
        "import cv2\n",
        "import datetime\n",
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "from base64 import b64encode\n",
        "from IPython import display\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from types import SimpleNamespace\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "def guidance_from_string(str: str) -> generation.GuidancePreset:\n",
        "    mappings = {\n",
        "        \"None\": generation.GUIDANCE_PRESET_NONE,\n",
        "        \"Simple\": generation.GUIDANCE_PRESET_SIMPLE,\n",
        "        \"FastBlue\": generation.GUIDANCE_PRESET_FAST_BLUE,\n",
        "        \"FastGreen\": generation.GUIDANCE_PRESET_FAST_GREEN,\n",
        "    }\n",
        "    repr = mappings.get(str, None)\n",
        "    if repr is None:\n",
        "        raise Exception(\"invalid guider provided\")\n",
        "    return repr\n",
        "\n",
        "def image_gen(\n",
        "    stub:generation_grpc.GenerationServiceStub, \n",
        "    width:int, height:int, \n",
        "    prompts:List[str], weights:List[str], \n",
        "    steps:int, seed:int, cfg_scale:float, \n",
        "    sampler: generation.DiffusionSampler,\n",
        "    init_image:np.ndarray, init_strength:float,\n",
        "    init_noise_scale: float = 1.0,\n",
        "    guidance_preset: generation.GuidancePreset = generation.GUIDANCE_PRESET_NONE,\n",
        "    guidance_cuts: int = 0,\n",
        "    guidance_strength: float = 0.0,\n",
        ") -> np.ndarray:\n",
        "\n",
        "    p = [generation.Prompt(text=prompt, parameters=generation.PromptParameters(weight=weight)) for prompt,weight in zip(prompts, weights)]\n",
        "    if init_image is not None:\n",
        "        p.append(image_to_prompt(init_image))\n",
        "\n",
        "    step_parameters = {\n",
        "        \"scaled_step\": 0,\n",
        "        \"sampler\": generation.SamplerParameters(cfg_scale=cfg_scale, init_noise_scale=init_noise_scale),\n",
        "    }\n",
        "\n",
        "    begin_schedule = 1.0 if init_image is None else 1.0-init_strength\n",
        "    if begin_schedule != 1.0:\n",
        "        step_parameters[\"schedule\"] = generation.ScheduleParameters(\n",
        "            start=begin_schedule\n",
        "        )\n",
        "\n",
        "    if guidance_preset is not generation.GUIDANCE_PRESET_NONE:\n",
        "        guidance_prompt = None\n",
        "        guiders = None\n",
        "        if guidance_cuts:\n",
        "            cutouts = generation.CutoutParameters(count=guidance_cuts)\n",
        "        else:\n",
        "            cutouts = None\n",
        "        step_parameters[\"guidance\"] = generation.GuidanceParameters(\n",
        "            guidance_preset=guidance_preset,\n",
        "            instances=[\n",
        "                generation.GuidanceInstanceParameters(\n",
        "                    guidance_strength=guidance_strength,\n",
        "                    models=guiders,\n",
        "                    cutouts=cutouts,\n",
        "                    prompt=guidance_prompt,\n",
        "                )\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    imageParams = {\n",
        "        \"height\": height,\n",
        "        \"width\": width,\n",
        "        \"seed\": [seed],\n",
        "        \"steps\": steps,\n",
        "        \"parameters\": [generation.StepParameter(**step_parameters)],\n",
        "    }\n",
        "    rq = generation.Request(\n",
        "        engine_id=GENERATE_ENGINE_ID,\n",
        "        prompt=p,\n",
        "        image=generation.ImageParameters(**imageParams)\n",
        "    )        \n",
        "    rq.image.transform.diffusion = sampler\n",
        "\n",
        "    for resp in stub.Generate(rq, wait_for_ready=True):\n",
        "        for artifact in resp.artifacts:\n",
        "            if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "                nparr = np.frombuffer(artifact.binary, np.uint8)\n",
        "                return cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "def image_inpaint(\n",
        "    stub:generation_grpc.GenerationServiceStub, image:np.ndarray, mask:np.ndarray,\n",
        "    prompts:List[str], weights:List[float], steps:int, seed:int, cfg_scale:float,\n",
        "    blur_ks:int = 11\n",
        ") -> np.ndarray:\n",
        "    width, height = image.shape[1], image.shape[0]\n",
        "    mask = cv2.GaussianBlur(mask, (blur_ks,blur_ks), 0)\n",
        "\n",
        "    p = [generation.Prompt(text=prompt, parameters=generation.PromptParameters(weight=weight)) for prompt,weight in zip(prompts, weights)]\n",
        "    p.extend([\n",
        "        image_to_prompt(image),\n",
        "        image_to_prompt_mask(mask)\n",
        "    ])\n",
        "    rq = generation.Request(\n",
        "        engine_id=INPAINT_ENGINE_ID,\n",
        "        prompt=p,\n",
        "        image=generation.ImageParameters(height=height, width=width, steps=steps, seed=[seed])\n",
        "    )\n",
        "    rq.image.parameters.append(\n",
        "        generation.StepParameter(\n",
        "            schedule=generation.ScheduleParameters(start=0.99),\n",
        "            sampler=generation.SamplerParameters(cfg_scale=cfg_scale)\n",
        "        )\n",
        "    )\n",
        "    for resp in stub.Generate(rq, wait_for_ready=True):\n",
        "        for artifact in resp.artifacts:\n",
        "            if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "                nparr = np.frombuffer(artifact.binary, np.uint8)\n",
        "                return cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "    \n",
        "    raise Exception(f\"no image artifact returned from inpaint request\")\n",
        "\n",
        "def image_mix(img_a: np.ndarray, img_b: np.ndarray, tween: float) -> np.ndarray:\n",
        "    assert(img_a.shape == img_b.shape)\n",
        "    return (img_a.astype(float)*(1.0-tween) + img_b.astype(float)*tween).astype(img_a.dtype)\n",
        "\n",
        "def image_to_jpg_bytes(image: np.ndarray, quality: int=90):\n",
        "    return cv2.imencode('.jpg', image, [int(cv2.IMWRITE_JPEG_QUALITY), quality])[1].tobytes()\n",
        "\n",
        "def image_to_png_bytes(image: np.ndarray):\n",
        "    return cv2.imencode('.png', image)[1].tobytes()\n",
        "\n",
        "def image_to_prompt(image: np.ndarray) -> generation.Prompt:\n",
        "    return generation.Prompt(\n",
        "        parameters=generation.PromptParameters(init=True),\n",
        "        artifact=generation.Artifact(\n",
        "            type=generation.ARTIFACT_IMAGE,\n",
        "            binary=image_to_png_bytes(image)))\n",
        "\n",
        "def image_to_prompt_mask(image: np.ndarray) -> generation.Prompt:\n",
        "    mask = image_to_prompt(image)\n",
        "    mask.artifact.type = generation.ARTIFACT_MASK\n",
        "    return mask\n",
        "\n",
        "def image_xform(\n",
        "    stub:generation_grpc.GenerationServiceStub, \n",
        "    images:List[np.ndarray], \n",
        "    ops:List[generation.TransformOperation]\n",
        ") -> Tuple[List[np.ndarray], np.ndarray]:\n",
        "    assert(len(images))\n",
        "    transforms = generation.TransformSequence(operations=ops)\n",
        "    p = [image_to_prompt(image) for image in images]\n",
        "    rq = generation.Request(\n",
        "        engine_id=TRANSFORM_ENGINE_ID,\n",
        "        prompt=p,\n",
        "        image=generation.ImageParameters(transform=generation.TransformType(sequence=transforms)),\n",
        "    )\n",
        "\n",
        "    images, mask = [], None\n",
        "    for resp in stub.Generate(rq, wait_for_ready=True):\n",
        "        for artifact in resp.artifacts:\n",
        "            if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "                nparr = np.frombuffer(artifact.binary, np.uint8)\n",
        "                images.append(cv2.imdecode(nparr, cv2.IMREAD_COLOR))\n",
        "            elif artifact.type == generation.ARTIFACT_MASK:\n",
        "                nparr = np.frombuffer(artifact.binary, np.uint8)\n",
        "                mask = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "    return images, mask\n",
        "\n",
        "def key_frame_inbetweens(key_frames, max_frames, integer=False, interp_method='Linear'):\n",
        "    key_frame_series = pd.Series([np.nan for a in range(max_frames)])\n",
        "\n",
        "    for i, value in key_frames.items():\n",
        "        key_frame_series[i] = value\n",
        "    key_frame_series = key_frame_series.astype(float)\n",
        "    \n",
        "    if interp_method == 'Cubic' and len(key_frames.items()) <= 3:\n",
        "      interp_method = 'Quadratic'    \n",
        "    if interp_method == 'Quadratic' and len(key_frames.items()) <= 2:\n",
        "      interp_method = 'Linear'\n",
        "          \n",
        "    key_frame_series[0] = key_frame_series[key_frame_series.first_valid_index()]\n",
        "    key_frame_series[max_frames-1] = key_frame_series[key_frame_series.last_valid_index()]\n",
        "    key_frame_series = key_frame_series.interpolate(method=interp_method.lower(), limit_direction='both')\n",
        "    if integer:\n",
        "        return key_frame_series.astype(int)\n",
        "    return key_frame_series\n",
        "\n",
        "def key_frame_parse(string, prompt_parser=None):\n",
        "    pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n",
        "    frames = dict()\n",
        "    for match_object in re.finditer(pattern, string):\n",
        "        frame = int(match_object.groupdict()['frame'])\n",
        "        param = match_object.groupdict()['param']\n",
        "        if prompt_parser:\n",
        "            frames[frame] = prompt_parser(param)\n",
        "        else:\n",
        "            frames[frame] = param\n",
        "    if frames == {} and len(string) != 0:\n",
        "        raise RuntimeError('Key Frame string not correctly formatted')\n",
        "    return frames\n",
        "\n",
        "def sampler_from_string(str: str) -> generation.DiffusionSampler:\n",
        "    mappings = {\n",
        "        \"DDIM\": generation.SAMPLER_DDIM,\n",
        "        \"PLMS\": generation.SAMPLER_DDPM,\n",
        "        \"K_euler\": generation.SAMPLER_K_EULER,\n",
        "        \"K_euler_ancestral\": generation.SAMPLER_K_EULER_ANCESTRAL,\n",
        "        \"K_heun\": generation.SAMPLER_K_HEUN,\n",
        "        \"K_dpm_2\": generation.SAMPLER_K_DPM_2,\n",
        "        \"K_dpm_2_ancestral\": generation.SAMPLER_K_DPM_2_ANCESTRAL,\n",
        "        \"K_lms\": generation.SAMPLER_K_LMS,\n",
        "    }\n",
        "    repr = mappings.get(str, None)\n",
        "    if not repr:\n",
        "        raise Exception(\"invalid sampler provided\")\n",
        "    return repr\n",
        "\n",
        "def warp2d_op(dx:float, dy:float, rotate:float, scale:float, border:str) -> generation.TransformOperation:\n",
        "    warp2d = generation.TransformWarp2d()\n",
        "\n",
        "    if border == 'replicate': warp2d.border_mode = generation.BORDER_REPLICATE\n",
        "    elif border == 'reflect': warp2d.border_mode = generation.BORDER_REFLECT\n",
        "    elif border == 'wrap': warp2d.border_mode = generation.BORDER_WRAP\n",
        "    elif border == 'zero': warp2d.border_mode = generation.BORDER_ZERO\n",
        "    else: raise Exception(f\"invalid 2d border mode {border}\")\n",
        "\n",
        "    warp2d.rotate = rotate\n",
        "    warp2d.scale = scale\n",
        "    warp2d.translate_x = dx\n",
        "    warp2d.translate_y = dy\n",
        "    return generation.TransformOperation(warp2d=warp2d)\n",
        "\n",
        "def warp3d_op(\n",
        "    dx:float, dy:float, dz:float, rx:float, ry:float, rz:float,\n",
        "    near:float, far:float, fov:float, border:str\n",
        ") -> generation.TransformOperation:\n",
        "    warp3d = generation.TransformWarp3d()\n",
        "\n",
        "    if border == 'replicate': warp3d.border_mode = generation.BORDER_REPLICATE\n",
        "    elif border == 'reflect': warp3d.border_mode = generation.BORDER_REFLECT\n",
        "    elif border == 'zero': warp3d.border_mode = generation.BORDER_ZERO\n",
        "    else: raise Exception(f\"invalid 3d border mode {border}\")\n",
        "\n",
        "    warp3d.translate_x = dx\n",
        "    warp3d.translate_y = dy\n",
        "    warp3d.translate_z = dz\n",
        "    warp3d.rotate_x = rx\n",
        "    warp3d.rotate_y = ry\n",
        "    warp3d.rotate_z = rz\n",
        "    warp3d.near_plane = near\n",
        "    warp3d.far_plane = far\n",
        "    warp3d.fov = fov\n",
        "    return generation.TransformOperation(warp3d=warp3d)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ldUAFmur3prH"
      },
      "outputs": [],
      "source": [
        "#@title Settings\n",
        "\n",
        "def Args():\n",
        "\n",
        "    #@markdown ####**Settings:**\n",
        "    W = 512 #@param\n",
        "    H = 512 #@param\n",
        "    W, H = map(lambda x: x - x % 64, (W, H))  # resize to integer multiple of 64\n",
        "    sampler = 'K_dpm_2_ancestral' #@param [\"DDIM\", \"PLMS\", \"K_euler\", \"K_euler_ancestral\", \"K_heun\", \"K_dpm_2\", \"K_dpm_2_ancestral\", \"K_lms\"]\n",
        "    seed = 42 #@param\n",
        "    cfg_scale = 7 #@param {type:\"number\"}\n",
        "    clip_guidance = 'FastBlue' #@param [\"None\", \"Simple\", \"FastBlue\", \"FastGreen\"]\n",
        "\n",
        "    #@markdown ####**Animation Settings:**\n",
        "    animation_mode = '3D' #@param ['2D', '3D', 'Video Input'] {type:'string'}\n",
        "    max_frames = 60 #@param {type:\"number\"}\n",
        "    border = 'replicate' #@param ['reflect', 'replicate', 'wrap', 'zero'] {type:'string'}\n",
        "    inpaint_border = False #@param {type:\"boolean\"}\n",
        "    interpolate_prompts = False #@param {type:\"boolean\"}\n",
        "    locked_seed = False #@param {type:\"boolean\"}\n",
        "\n",
        "    #@markdown ####**Key framed value curves:**\n",
        "    angle = \"0:(1)\" #@param {type:\"string\"}\n",
        "    zoom = \"0:(1.05)\" #@param {type:\"string\"}\n",
        "    translation_x = \"0:(0)\" #@param {type:\"string\"}\n",
        "    translation_y = \"0:(0)\" #@param {type:\"string\"}\n",
        "    translation_z = \"0:(5)\" #@param {type:\"string\"}\n",
        "    rotation_x = \"0:(0)\" #@param {type:\"string\"}\n",
        "    rotation_y = \"0:(0)\" #@param {type:\"string\"}\n",
        "    rotation_z = \"0:(1)\" #@param {type:\"string\"}\n",
        "    brightness_curve = \"0: (1.0)\" #@param {type:\"string\"}\n",
        "    contrast_curve = \"0: (1.0)\" #@param {type:\"string\"}\n",
        "    noise_curve = \"0:(0.0)\" # likely to be removed, still hidden here for potential experiments\n",
        "    noise_scale_curve = \"0:(1.02)\" #@param {type:\"string\"}\n",
        "    steps_curve = \"0:(50)\" #@param {type:\"string\"}\n",
        "    strength_curve = \"0:(0.65)\" #@param {type:\"string\"}\n",
        "\n",
        "    #@markdown ####**Coherence:**\n",
        "    color_coherence = True #@param {type:\"boolean\"}\n",
        "    diffusion_cadence = '3' #@param ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16'] {type:'string'}\n",
        "\n",
        "    #@markdown ####**3D Depth Warping:**\n",
        "    #use_depth_warping = True #@param {type:\"boolean\"}\n",
        "    midas_weight = 0.3 #@param {type:\"number\"}\n",
        "    near_plane = 200\n",
        "    far_plane = 10000\n",
        "    fov = 20 #@param {type:\"number\"}\n",
        "    save_depth_maps = False #@param {type:\"boolean\"}\n",
        "\n",
        "    #@markdown ####**Video Input:**\n",
        "    video_init_path ='/content/video_in.mp4' #@param {type:\"string\"}\n",
        "    extract_nth_frame = 4 #@param {type:\"number\"}\n",
        "    video_mix_in = 0.02 #@param {type:\"number\"}\n",
        "    video_flow_warp = True #@param {type:\"boolean\"}\n",
        "\n",
        "    return locals()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SudvbZG3prI"
      },
      "source": [
        "### Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT9slDSw3prJ"
      },
      "outputs": [],
      "source": [
        "animation_prompts = {\n",
        "    0: \"a painting of a delicious cheeseburger by Tyler Edlin\",\n",
        "    24: \"a painting of the the answer to life the universe and everything by Tyler Edlin\",\n",
        "}\n",
        "\n",
        "negative_prompt = \"\"\n",
        "negative_prompt_weight = -1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rpqv6t303prJ"
      },
      "outputs": [],
      "source": [
        "#@title Render the animation\n",
        "\n",
        "def display_frame(image: np.ndarray):\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)))\n",
        "\n",
        "def get_animation_prompts_weights(frame_idx: int, key_frame_values: List[int], interp: bool) -> Tuple[List[str], List[float]]:\n",
        "    idx = bisect.bisect_right(key_frame_values, frame_idx)\n",
        "    prev, next = idx - 1, idx\n",
        "    if not interp:\n",
        "        return [animation_prompts[key_frame_values[min(len(key_frame_values)-1, prev)]]], [1.0]\n",
        "    elif next == len(key_frame_values):\n",
        "        return [animation_prompts[key_frame_values[-1]]], [1.0]\n",
        "    else:\n",
        "        tween = (frame_idx - key_frame_values[prev]) / (key_frame_values[next] - key_frame_values[prev])\n",
        "        return [animation_prompts[key_frame_values[prev]], animation_prompts[key_frame_values[next]]], [1.0 - tween, tween]\n",
        "\n",
        "def render_animation(args, out_dir):\n",
        "    # choose a random seed if set to 0\n",
        "    if args.seed == 0:\n",
        "        args.seed = random.randint(0, 2**32 - 1)\n",
        "\n",
        "    # save settings for the animation\n",
        "    settings_filename = os.path.join(out_dir, f\"{timestring}_settings.txt\")\n",
        "    with open(settings_filename, \"w+\", encoding=\"utf-8\") as f:\n",
        "        json.dump(vars(args), f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # expand key frames\n",
        "    angle_series = key_frame_inbetweens(key_frame_parse(args.angle), args.max_frames)\n",
        "    zoom_series = key_frame_inbetweens(key_frame_parse(args.zoom), args.max_frames)\n",
        "    translation_x_series = key_frame_inbetweens(key_frame_parse(args.translation_x), args.max_frames)\n",
        "    translation_y_series = key_frame_inbetweens(key_frame_parse(args.translation_y), args.max_frames)\n",
        "    translation_z_series = key_frame_inbetweens(key_frame_parse(args.translation_z), args.max_frames)\n",
        "    rotation_x_series = key_frame_inbetweens(key_frame_parse(args.rotation_x), args.max_frames)\n",
        "    rotation_y_series = key_frame_inbetweens(key_frame_parse(args.rotation_y), args.max_frames)\n",
        "    rotation_z_series = key_frame_inbetweens(key_frame_parse(args.rotation_z), args.max_frames)\n",
        "    brightness_series = key_frame_inbetweens(key_frame_parse(args.brightness_curve), args.max_frames)\n",
        "    contrast_series = key_frame_inbetweens(key_frame_parse(args.contrast_curve), args.max_frames)\n",
        "    noise_series = key_frame_inbetweens(key_frame_parse(args.noise_curve), args.max_frames)\n",
        "    noise_scale_series = key_frame_inbetweens(key_frame_parse(args.noise_scale_curve), args.max_frames)\n",
        "    steps_series = key_frame_inbetweens(key_frame_parse(args.steps_curve), args.max_frames)\n",
        "    strength_series = key_frame_inbetweens(key_frame_parse(args.strength_curve), args.max_frames)\n",
        "\n",
        "    # prepare sorted list of key frames\n",
        "    key_frame_values = sorted(list(animation_prompts.keys()))\n",
        "    if key_frame_values[0] != 0:\n",
        "        raise ValueError(\"First keyframe must be 0\")\n",
        "    if len(key_frame_values) != len(set(key_frame_values)):\n",
        "        raise ValueError(\"Duplicate keyframes are not allowed!\")\n",
        "\n",
        "    # diffusion performed every N frames. two prior diffused frames\n",
        "    # are transformed and blended between to produce each output frame\n",
        "    diffusion_cadence = max(1, int(args.diffusion_cadence))\n",
        "    prior_frames = []\n",
        "\n",
        "    # load input video\n",
        "    video_in = args.video_init_path if args.animation_mode == 'Video Input' else None\n",
        "    video_reader = None if video_in is None else cv2.VideoCapture(video_in)\n",
        "    video_extract_nth = args.extract_nth_frame\n",
        "    video_prev_frame = None\n",
        "    if video_reader is not None:\n",
        "        success, image = video_reader.read()\n",
        "        if not success:\n",
        "            raise Exception(f\"Failed to read first frame from {video_in}\")\n",
        "        video_prev_frame = cv2.resize(image, (args.W, args.H), interpolation=cv2.INTER_LANCZOS4)\n",
        "        prior_frames = [video_prev_frame, video_prev_frame]\n",
        "\n",
        "    color_match_image = None # optional target for color matching\n",
        "    inpaint_mask = None      # optional mask of revealed areas\n",
        "    seed = args.seed\n",
        "\n",
        "    for frame_idx in tqdm(range(args.max_frames)):\n",
        "        steps = int(steps_series[frame_idx])\n",
        "\n",
        "        # fetch set of prompts and weights for this frame\n",
        "        prompts, weights = get_animation_prompts_weights(frame_idx, key_frame_values, interp=args.interpolate_prompts)\n",
        "        if len(negative_prompt) and negative_prompt_weight != 0.0:\n",
        "            prompts.append(negative_prompt)\n",
        "            weights.append(-abs(negative_prompt_weight))\n",
        "\n",
        "        # apply transformation to prior frames\n",
        "        if len(prior_frames):\n",
        "            ops = []\n",
        "            if args.save_depth_maps or args.animation_mode == '3D':\n",
        "                ops.append(generation.TransformOperation(                    \n",
        "                    depth_calc=generation.TransformDepthCalc(\n",
        "                        blend_weight=args.midas_weight,\n",
        "                        export=args.save_depth_maps\n",
        "                    )\n",
        "                ))\n",
        "            if args.animation_mode == '2D':\n",
        "                ops.append(warp2d_op(\n",
        "                    translation_x_series[frame_idx], \n",
        "                    translation_y_series[frame_idx], \n",
        "                    angle_series[frame_idx], \n",
        "                    zoom_series[frame_idx], \n",
        "                    args.border\n",
        "                ))\n",
        "            elif args.animation_mode == '3D':\n",
        "                ops.append(warp3d_op(\n",
        "                    translation_x_series[frame_idx], \n",
        "                    translation_y_series[frame_idx], \n",
        "                    translation_z_series[frame_idx], \n",
        "                    rotation_x_series[frame_idx], \n",
        "                    rotation_y_series[frame_idx], \n",
        "                    rotation_z_series[frame_idx], \n",
        "                    args.near_plane, args.far_plane, \n",
        "                    args.fov, args.border\n",
        "                ))\n",
        "            elif args.animation_mode == 'Video Input':\n",
        "                for i in range(video_extract_nth):\n",
        "                    success, video_next_frame = video_reader.read()\n",
        "                if success:\n",
        "                    video_next_frame = cv2.resize(video_next_frame, (args.W, args.H), interpolation=cv2.INTER_LANCZOS4)\n",
        "                    if args.video_flow_warp:\n",
        "                        ops.append(generation.TransformOperation(\n",
        "                            warp_flow=generation.TransformWarpFlow(\n",
        "                                prev_frame=generation.Artifact(type=generation.ARTIFACT_IMAGE, binary=image_to_jpg_bytes(video_prev_frame)),\n",
        "                                next_frame=generation.Artifact(type=generation.ARTIFACT_IMAGE, binary=image_to_jpg_bytes(video_next_frame)),\n",
        "                            )\n",
        "                        ))\n",
        "                    video_prev_frame = video_next_frame\n",
        "                    color_match_image = video_next_frame\n",
        "            if len(ops):\n",
        "                prior_frames, mask = image_xform(stub, prior_frames, ops)\n",
        "                inpaint_mask = mask if args.inpaint_border else None\n",
        "\n",
        "                depth_map = prior_frames.pop(0) if len(prior_frames) == 3 else None\n",
        "                if depth_map is not None and args.save_depth_maps:\n",
        "                    cv2.imwrite(os.path.join(out_dir, f\"depth_{frame_idx:05d}.png\"), depth_map)\n",
        "\n",
        "                if inpaint_mask is not None:\n",
        "                    for i in range(len(prior_frames)):\n",
        "                        prior_frames[i] = image_inpaint(stub, prior_frames[i], inpaint_mask, prompts, weights, steps//2, seed, args.cfg_scale)\n",
        "                    inpaint_mask = None\n",
        "\n",
        "        # either run diffusion or emit an inbetween frame\n",
        "        if frame_idx % diffusion_cadence == 0:\n",
        "            if inpaint_mask is not None:\n",
        "                prior_frames[-1] = image_inpaint(stub, prior_frames[-1], inpaint_mask, prompts, weights, steps//2, seed, args.cfg_scale)\n",
        "                inpaint_mask = None\n",
        "            strength = strength_series[frame_idx]\n",
        "\n",
        "            # apply additional noising and color matching to previous frame to use as init\n",
        "            init_image = prior_frames[-1] if len(prior_frames) and strength > 0 else None\n",
        "            if init_image is not None:\n",
        "                noise = noise_series[frame_idx]\n",
        "                brightness = brightness_series[frame_idx]\n",
        "                contrast = contrast_series[frame_idx]\n",
        "                ops = []\n",
        "                if args.color_coherence and color_match_image is not None:                    \n",
        "                    ops.append(generation.TransformOperation(color_match=generation.TransformColorMatch(\n",
        "                        color_mode=generation.COLOR_MATCH_LAB,\n",
        "                        image=generation.Artifact(type=generation.ARTIFACT_IMAGE, binary=image_to_jpg_bytes(color_match_image))\n",
        "                    )))\n",
        "                if args.video_mix_in > 0 and video_prev_frame is not None:\n",
        "                    ops.append(generation.TransformOperation(blend=generation.TransformBlend(\n",
        "                        amount=args.video_mix_in, \n",
        "                        target=generation.Artifact(type=generation.ARTIFACT_IMAGE, binary=image_to_jpg_bytes(video_prev_frame))\n",
        "                    )))\n",
        "                if brightness != 1.0 or contrast != 1.0:\n",
        "                    ops.append(generation.TransformOperation(contrast=generation.TransformContrast(\n",
        "                        brightness=brightness, contrast=contrast\n",
        "                    )))\n",
        "                if noise > 0:\n",
        "                    ops.append(generation.TransformOperation(add_noise=generation.TransformAddNoise(amount=noise, seed=seed)))\n",
        "                if len(ops):\n",
        "                    init_image = image_xform(stub, [init_image], ops)[0][0]\n",
        "\n",
        "            # generate the next frame\n",
        "            sampler = sampler_from_string(args.sampler)\n",
        "            guidance = guidance_from_string(args.clip_guidance)\n",
        "            noise_scale = noise_scale_series[frame_idx]\n",
        "            image = image_gen(\n",
        "                stub, \n",
        "                args.W, args.H, \n",
        "                prompts, weights, \n",
        "                steps, seed, args.cfg_scale, sampler, \n",
        "                init_image, strength,\n",
        "                init_noise_scale=noise_scale, \n",
        "                guidance_preset=guidance\n",
        "            )\n",
        "\n",
        "            if color_match_image is None:\n",
        "                color_match_image = image\n",
        "            if not len(prior_frames):\n",
        "                prior_frames = [image, image]\n",
        "            \n",
        "            cv2.imwrite(os.path.join(out_dir, f'frame_{frame_idx:05}.png'), prior_frames[1])\n",
        "            display_frame(prior_frames[1])\n",
        "            prior_frames[0] = prior_frames[1]\n",
        "            prior_frames[1] = image            \n",
        "        else:\n",
        "            # smoothly blend between prior frames\n",
        "            tween = (frame_idx % diffusion_cadence) / float(diffusion_cadence)\n",
        "            t = image_mix(prior_frames[0], prior_frames[1], tween)\n",
        "            cv2.imwrite(os.path.join(out_dir, f'frame_{frame_idx:05}.png'), t)\n",
        "            display_frame(t)\n",
        "\n",
        "        if not args.locked_seed:\n",
        "            seed += 1\n",
        "\n",
        "# create folder for frames output\n",
        "timestring = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "out_dir = os.path.join(outputs_path, timestring)\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "print(f\"Saving animation frames to {out_dir}...\")\n",
        "\n",
        "render_animation(SimpleNamespace(**Args()), out_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aWhJnLNX3prL"
      },
      "outputs": [],
      "source": [
        "#@title Create video from frames\n",
        "\n",
        "fps = 12 #@param {type:\"number\"}\n",
        "\n",
        "image_path = os.path.join(out_dir, \"frame_%05d.png\")\n",
        "mp4_path = os.path.join(out_dir, f\"{timestring}.mp4\")\n",
        "\n",
        "print(f\"Compiling animation frames to {mp4_path}...\")\n",
        "\n",
        "cmd = [\n",
        "    'ffmpeg',\n",
        "    '-y',\n",
        "    '-vcodec', 'png',\n",
        "    '-r', str(fps),\n",
        "    '-start_number', str(0),\n",
        "    '-i', image_path,\n",
        "    '-c:v', 'libx264',\n",
        "    '-vf',\n",
        "    f'fps={fps}',\n",
        "    '-pix_fmt', 'yuv420p',\n",
        "    '-crf', '17',\n",
        "    '-preset', 'veryfast',\n",
        "    mp4_path\n",
        "]\n",
        "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "stdout, stderr = process.communicate()\n",
        "if process.returncode != 0:\n",
        "    print(stderr)\n",
        "    raise RuntimeError(stderr)\n",
        "\n",
        "mp4 = open(mp4_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "display.display( display.HTML(f'<video controls loop><source src=\"{data_url}\" type=\"video/mp4\"></video>') )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d4dd9c310c32a31bb53615812f2f2c6cba010b7aa4dfb14e2b192e650667fecd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
