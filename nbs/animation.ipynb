{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWXrHouW3pq_"
      },
      "source": [
        "# Animation example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "jCZ-IphH3prD"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    outputs_path = \"/content/gdrive/MyDrive/AI/StabilityAnimations\"\n",
        "    !mkdir -p $outputs_path\n",
        "except:\n",
        "    outputs_path = \".\"\n",
        "print(f\"Animations will be saved to {outputs_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "zj56t6tc3prF"
      },
      "outputs": [],
      "source": [
        "#@title Connect to the Stability API\n",
        "import grpc\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "\n",
        "path = Path('stability-sdk')\n",
        "if path.exists():\n",
        "    shutil.rmtree(path)\n",
        "    !pip uninstall -y stability-sdk\n",
        "\n",
        "\n",
        "!git clone -b anima --recurse-submodules https://github.com/Stability-AI/stability-sdk\n",
        "!touch ./stability-sdk/src/stability_sdk/interfaces/__init__.py\n",
        "!pip install ./stability-sdk\n",
        "\n",
        "\n",
        "from stability_sdk import client\n",
        "from stability_sdk.utils import (\n",
        "    color_match_from_string,\n",
        "    sampler_from_string,\n",
        "    key_frame_inbetweens,\n",
        "    key_frame_parse,\n",
        "    guidance_from_string,\n",
        "    #curve_to_series,\n",
        "    image_mix,\n",
        "    image_to_jpg_bytes,\n",
        "    image_to_png_bytes,\n",
        "    image_to_prompt,\n",
        "    image_xform,\n",
        "    warp2d_op,\n",
        "    warp3d_op,\n",
        ")\n",
        "\n",
        "\n",
        "from stability_sdk.client import generation, generation_grpc # not a huge fan of this but at least it works\n",
        "\n",
        "\n",
        "# GRPC endpoint and engines\n",
        "GRPC_HOST = \"\" #@param {type:\"string\"}\n",
        "API_KEY = \"\" #@param {type:\"string\"}\n",
        "GENERATE_ENGINE_ID = 'stable-diffusion-v1-5'\n",
        "INPAINT_ENGINE_ID = 'stable-diffusion-v1-5'\n",
        "TRANSFORM_ENGINE_ID = 'transform-server-v1'\n",
        "\n",
        "\n",
        "# Connect to Stability API\n",
        "stub = client.open_channel(GRPC_HOST, api_key=API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "3Rf9Fcqi3prF"
      },
      "outputs": [],
      "source": [
        "#@title Code definitions\n",
        "\n",
        "import bisect\n",
        "import cv2\n",
        "import datetime\n",
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "from base64 import b64encode\n",
        "from collections import OrderedDict\n",
        "from IPython import display\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from types import SimpleNamespace\n",
        "from typing import List, Tuple\n",
        "\n",
        "from stability_sdk.client import (\n",
        "    image_gen,\n",
        "    image_inpaint,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "ldUAFmur3prH"
      },
      "outputs": [],
      "source": [
        "#@title Settings\n",
        "\n",
        "def Args():\n",
        "\n",
        "    #@markdown ####**Settings:**\n",
        "    W = 512 #@param\n",
        "    H = 512 #@param\n",
        "    W, H = map(lambda x: x - x % 64, (W, H))  # resize to integer multiple of 64\n",
        "    sampler = 'K_euler_ancestral' #@param [\"DDIM\", \"PLMS\", \"K_euler\", \"K_euler_ancestral\", \"K_heun\", \"K_dpm_2\", \"K_dpm_2_ancestral\", \"K_lms\"]\n",
        "    seed = -1 #@param\n",
        "    cfg_scale = 7 #@param {type:\"number\"}\n",
        "    clip_guidance = 'FastBlue' #@param [\"None\", \"Simple\", \"FastBlue\", \"FastGreen\"]\n",
        "\n",
        "    #@markdown ####**Animation Settings:**\n",
        "    animation_mode = '3D' #@param ['2D', '3D', 'Video Input'] {type:'string'}\n",
        "    max_frames = 60 #@param {type:\"number\"}\n",
        "    border = 'replicate' #@param ['reflect', 'replicate', 'wrap', 'zero'] {type:'string'}\n",
        "    inpaint_border = False #@param {type:\"boolean\"}\n",
        "    interpolate_prompts = False #@param {type:\"boolean\"}\n",
        "    locked_seed = False #@param {type:\"boolean\"}\n",
        "\n",
        "    #@markdown ####**Key framed value curves:**\n",
        "    angle = \"0:(1)\" #@param {type:\"string\"}\n",
        "    zoom = \"0:(1.05)\" #@param {type:\"string\"}\n",
        "    translation_x = \"0:(0)\" #@param {type:\"string\"}\n",
        "    translation_y = \"0:(0)\" #@param {type:\"string\"}\n",
        "    translation_z = \"0:(5)\" #@param {type:\"string\"}\n",
        "    rotation_x = \"0:(0)\" #@param {type:\"string\"}\n",
        "    rotation_y = \"0:(0)\" #@param {type:\"string\"}\n",
        "    rotation_z = \"0:(1)\" #@param {type:\"string\"}\n",
        "    brightness_curve = \"0: (1.0)\" #@param {type:\"string\"}\n",
        "    contrast_curve = \"0: (1.0)\" #@param {type:\"string\"}\n",
        "    noise_curve = \"0:(0.0)\" # likely to be removed, still hidden here for potential experiments\n",
        "    noise_scale_curve = \"0:(1.02)\" #@param {type:\"string\"}\n",
        "    steps_curve = \"0:(50)\" #@param {type:\"string\"}\n",
        "    strength_curve = \"0:(0.65)\" #@param {type:\"string\"}\n",
        "\n",
        "    #@markdown ####**Coherence:**\n",
        "    color_coherence = 'LAB' #@param ['None', 'HSV', 'LAB', 'RGB'] {type:'string'}\n",
        "    diffusion_cadence_curve = \"0:(4)\" #@param {type:\"string\"}\n",
        "\n",
        "    #@markdown ####**3D Depth Warping:**\n",
        "    #use_depth_warping = True #@param {type:\"boolean\"}\n",
        "    midas_weight = 0.3 #@param {type:\"number\"}\n",
        "    near_plane = 200\n",
        "    far_plane = 10000\n",
        "    fov_curve = \"0:(25)\" #@param {type:\"string\"}\n",
        "    save_depth_maps = False #@param {type:\"boolean\"}\n",
        "\n",
        "    #@markdown ####**Video Input:**\n",
        "    video_init_path = '/content/video_in.mp4' #@param {type:\"string\"}\n",
        "    extract_nth_frame = 4 #@param {type:\"number\"}\n",
        "    video_mix_in_curve = \"0:(0.02)\" #@param {type:\"string\"}\n",
        "    video_flow_warp = True #@param {type:\"boolean\"}\n",
        "\n",
        "    return locals()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SudvbZG3prI"
      },
      "source": [
        "### Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FT9slDSw3prJ"
      },
      "outputs": [],
      "source": [
        "animation_prompts = {\n",
        "    0: \"a painting of a delicious cheeseburger by Tyler Edlin\",\n",
        "    24: \"a painting of the the answer to life the universe and everything by Tyler Edlin\",\n",
        "}\n",
        "\n",
        "negative_prompt = \"\"\n",
        "negative_prompt_weight = -1.0\n",
        "\n",
        "#####################\n",
        "\n",
        "\n",
        "def display_frame(image: np.ndarray):\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)))\n",
        "\n",
        "def get_animation_prompts_weights(frame_idx: int, key_frame_values: List[int], interp: bool) -> Tuple[List[str], List[float]]:\n",
        "    idx = bisect.bisect_right(key_frame_values, frame_idx)\n",
        "    prev, next = idx - 1, idx\n",
        "    if not interp:\n",
        "        return [animation_prompts[key_frame_values[min(len(key_frame_values)-1, prev)]]], [1.0]\n",
        "    elif next == len(key_frame_values):\n",
        "        return [animation_prompts[key_frame_values[-1]]], [1.0]\n",
        "    else:\n",
        "        tween = (frame_idx - key_frame_values[prev]) / (key_frame_values[next] - key_frame_values[prev])\n",
        "        return [animation_prompts[key_frame_values[prev]], animation_prompts[key_frame_values[next]]], [1.0 - tween, tween]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "Rpqv6t303prJ"
      },
      "outputs": [],
      "source": [
        "#@title Render the animation\n",
        "\n",
        "class Animator:\n",
        "    def __init__(self, args, out_dir):\n",
        "        self.args = args\n",
        "        self.out_dir = out_dir\n",
        "        self.save_settings()\n",
        "        self.setup_animation()\n",
        "\n",
        "    def save_settings(self):\n",
        "            # save settings for the animation\n",
        "            settings_filename = os.path.join(out_dir, f\"{timestring}_settings.txt\")\n",
        "            with open(settings_filename, \"w+\", encoding=\"utf-8\") as f:\n",
        "                save_dict = OrderedDict(vars(args))\n",
        "                for k in ['angle', 'zoom', 'translation_x', 'translation_y', 'translation_z', 'rotation_x', 'rotation_y', 'rotation_z']:\n",
        "                    save_dict.move_to_end(k, last=True)\n",
        "                save_dict['animation_prompts'] = animation_prompts\n",
        "                save_dict['negative_prompt'] = negative_prompt\n",
        "                save_dict['negative_prompt_weight'] = negative_prompt_weight\n",
        "                json.dump(save_dict, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    def setup_animation(self):\n",
        "        args = self.args\n",
        "        out_dir = self.out_dir\n",
        "\n",
        "        # change request for random seed into explicit value so it is saved to settings\n",
        "        if args.seed <= 0:\n",
        "            args.seed = random.randint(0, 2**32 - 1)\n",
        "\n",
        "        def curve_to_series(curve: str) -> List[float]:\n",
        "            return key_frame_inbetweens(key_frame_parse(curve), args.max_frames)    \n",
        "\n",
        "        self.frame_args = SimpleNamespace(**dict(\n",
        "            angle_series = curve_to_series(args.angle)\n",
        "            ,zoom_series = curve_to_series(args.zoom)\n",
        "            ,translation_x_series = curve_to_series(args.translation_x)\n",
        "            ,translation_y_series = curve_to_series(args.translation_y)\n",
        "            ,translation_z_series = curve_to_series(args.translation_z)\n",
        "            ,rotation_x_series = curve_to_series(args.rotation_x)\n",
        "            ,rotation_y_series = curve_to_series(args.rotation_y)\n",
        "            ,rotation_z_series = curve_to_series(args.rotation_z)\n",
        "            ,brightness_series = curve_to_series(args.brightness_curve)\n",
        "            ,contrast_series = curve_to_series(args.contrast_curve)\n",
        "            ,noise_series = curve_to_series(args.noise_curve)\n",
        "            ,noise_scale_series = curve_to_series(args.noise_scale_curve)\n",
        "            ,steps_series = curve_to_series(args.steps_curve)\n",
        "            ,strength_series = curve_to_series(args.strength_curve)\n",
        "            ,diffusion_cadence_series = curve_to_series(args.diffusion_cadence_curve)\n",
        "            ,fov_series = curve_to_series(args.fov_curve)\n",
        "            ,video_mix_in_series = curve_to_series(args.video_mix_in_curve)\n",
        "        ))\n",
        "\n",
        "        # prepare sorted list of key frames\n",
        "        key_frame_values = sorted(list(animation_prompts.keys()))\n",
        "        if key_frame_values[0] != 0:\n",
        "            raise ValueError(\"First keyframe must be 0\")\n",
        "        if len(key_frame_values) != len(set(key_frame_values)):\n",
        "            raise ValueError(\"Duplicate keyframes are not allowed!\")\n",
        "        self.keyframe_values = key_frame_values\n",
        "\n",
        "        # diffusion performed every N frames. two prior diffused frames\n",
        "        # are transformed and blended between to produce each output frame\n",
        "        #diffusion_cadence_ofs = 0\n",
        "        prior_frames = []\n",
        "\n",
        "        # load input video\n",
        "        video_in = args.video_init_path if args.animation_mode == 'Video Input' else None\n",
        "        video_reader = None if video_in is None else cv2.VideoCapture(video_in)\n",
        "        #video_extract_nth = args.extract_nth_frame\n",
        "        video_prev_frame = None\n",
        "        if video_reader is not None:\n",
        "            success, image = video_reader.read()\n",
        "            if not success:\n",
        "                raise Exception(f\"Failed to read first frame from {video_in}\")\n",
        "            video_prev_frame = cv2.resize(image, (args.W, args.H), interpolation=cv2.INTER_LANCZOS4)\n",
        "            prior_frames = [video_prev_frame, video_prev_frame]\n",
        "        \n",
        "        self.prior_frames = prior_frames\n",
        "        self.video_reader = video_reader\n",
        "        self.video_prev_frame = video_prev_frame\n",
        "\n",
        "    def build_prior_frame_transforms(self, prior_frames, frame_idx, args, prompts, weights, steps, seed, color_match_image):\n",
        "        video_extract_nth = args.extract_nth_frame\n",
        "        video_reader = self.video_reader\n",
        "\n",
        "        ops = []\n",
        "        if args.save_depth_maps or args.animation_mode == '3D':\n",
        "            ops.append(generation.TransformOperation(                    \n",
        "                depth_calc=generation.TransformDepthCalc(\n",
        "                    blend_weight=args.midas_weight,\n",
        "                    export=args.save_depth_maps\n",
        "                )\n",
        "            ))\n",
        "        if args.animation_mode == '2D':\n",
        "            ops.append(warp2d_op(\n",
        "                self.frame_args.translation_x_series[frame_idx], \n",
        "                self.frame_args.translation_y_series[frame_idx], \n",
        "                self.frame_args.angle_series[frame_idx], \n",
        "                self.frame_args.zoom_series[frame_idx], \n",
        "                args.border\n",
        "            ))\n",
        "        elif args.animation_mode == '3D':\n",
        "            ops.append(warp3d_op(\n",
        "                self.frame_args.translation_x_series[frame_idx], \n",
        "                self.frame_args.translation_y_series[frame_idx], \n",
        "                self.frame_args.translation_z_series[frame_idx], \n",
        "                self.frame_args.rotation_x_series[frame_idx], \n",
        "                self.frame_args.rotation_y_series[frame_idx], \n",
        "                self.frame_args.rotation_z_series[frame_idx], \n",
        "                args.near_plane, args.far_plane, \n",
        "                self.frame_args.fov_series[frame_idx], args.border\n",
        "            ))\n",
        "        elif args.animation_mode == 'Video Input':\n",
        "            for i in range(video_extract_nth):\n",
        "                success, video_next_frame = video_reader.read()\n",
        "            if success:\n",
        "                video_next_frame = cv2.resize(video_next_frame, (args.W, args.H), interpolation=cv2.INTER_LANCZOS4)\n",
        "                if args.video_flow_warp:\n",
        "                    ops.append(generation.TransformOperation(\n",
        "                        warp_flow=generation.TransformWarpFlow(\n",
        "                            prev_frame=generation.Artifact(type=generation.ARTIFACT_IMAGE, binary=image_to_jpg_bytes(video_prev_frame)),\n",
        "                            next_frame=generation.Artifact(type=generation.ARTIFACT_IMAGE, binary=image_to_jpg_bytes(video_next_frame)),\n",
        "                        )\n",
        "                    ))\n",
        "                video_prev_frame = video_next_frame\n",
        "                color_match_image = video_next_frame\n",
        "        if len(ops):\n",
        "            prior_frames, mask = image_xform(stub, prior_frames, ops, TRANSFORM_ENGINE_ID)\n",
        "            inpaint_mask = mask if args.inpaint_border else None\n",
        "\n",
        "            depth_map = prior_frames.pop(0) if len(prior_frames) == 3 else None\n",
        "            if depth_map is not None and args.save_depth_maps:\n",
        "                cv2.imwrite(os.path.join(out_dir, f\"depth_{frame_idx:05d}.png\"), depth_map)\n",
        "\n",
        "            if inpaint_mask is not None:\n",
        "                for i in range(len(prior_frames)):\n",
        "                    prior_frames[i] = image_inpaint(stub, prior_frames[i], inpaint_mask, prompts, weights, steps//2, seed, args.cfg_scale)\n",
        "                inpaint_mask = None\n",
        "        return ops, prior_frames, mask, inpaint_mask, color_match_image\n",
        "\n",
        "    def render_animation(self, args=None, out_dir=None):\n",
        "\n",
        "\n",
        "        if not args:\n",
        "            args = self.args\n",
        "        if not out_dir:\n",
        "            out_dir = self.out_dir\n",
        "        key_frame_values = self.keyframe_values\n",
        "        video_extract_nth = args.extract_nth_frame\n",
        "        seed = args.seed\n",
        "        color_match_image = None # optional target for color matching\n",
        "        inpaint_mask = None      # optional mask of revealed areas\n",
        "        diffusion_cadence_ofs = 0 # diffusion performed every N frames.\n",
        "\n",
        "        video_reader = self.video_reader\n",
        "        prior_frames = self.prior_frames\n",
        "        video_prev_frame = self.video_prev_frame\n",
        "        \n",
        "        for frame_idx in tqdm(range(args.max_frames)):\n",
        "\n",
        "\n",
        "            diffusion_cadence = max(1, int(self.frame_args.diffusion_cadence_series[frame_idx]))\n",
        "            steps = int(self.frame_args.steps_series[frame_idx])\n",
        "\n",
        "            # fetch set of prompts and weights for this frame\n",
        "            prompts, weights = get_animation_prompts_weights(frame_idx, key_frame_values, interp=args.interpolate_prompts)\n",
        "            if len(negative_prompt) and negative_prompt_weight != 0.0:\n",
        "                prompts.append(negative_prompt)\n",
        "                weights.append(-abs(negative_prompt_weight))\n",
        "\n",
        "            \"\"\"\n",
        "            # apply transformation to prior frames\n",
        "            if len(prior_frames):\n",
        "                ops = []\n",
        "                if args.save_depth_maps or args.animation_mode == '3D':\n",
        "                    ops.append(generation.TransformOperation(                    \n",
        "                        depth_calc=generation.TransformDepthCalc(\n",
        "                            blend_weight=args.midas_weight,\n",
        "                            export=args.save_depth_maps\n",
        "                        )\n",
        "                    ))\n",
        "                if args.animation_mode == '2D':\n",
        "                    ops.append(warp2d_op(\n",
        "                        self.frame_args.translation_x_series[frame_idx], \n",
        "                        self.frame_args.translation_y_series[frame_idx], \n",
        "                        self.frame_args.angle_series[frame_idx], \n",
        "                        self.frame_args.zoom_series[frame_idx], \n",
        "                        args.border\n",
        "                    ))\n",
        "                elif args.animation_mode == '3D':\n",
        "                    ops.append(warp3d_op(\n",
        "                        self.frame_args.translation_x_series[frame_idx], \n",
        "                        self.frame_args.translation_y_series[frame_idx], \n",
        "                        self.frame_args.translation_z_series[frame_idx], \n",
        "                        self.frame_args.rotation_x_series[frame_idx], \n",
        "                        self.frame_args.rotation_y_series[frame_idx], \n",
        "                        self.frame_args.rotation_z_series[frame_idx], \n",
        "                        args.near_plane, args.far_plane, \n",
        "                        self.frame_args.fov_series[frame_idx], args.border\n",
        "                    ))\n",
        "                elif args.animation_mode == 'Video Input':\n",
        "                    for i in range(video_extract_nth):\n",
        "                        success, video_next_frame = video_reader.read()\n",
        "                    if success:\n",
        "                        video_next_frame = cv2.resize(video_next_frame, (args.W, args.H), interpolation=cv2.INTER_LANCZOS4)\n",
        "                        if args.video_flow_warp:\n",
        "                            ops.append(generation.TransformOperation(\n",
        "                                warp_flow=generation.TransformWarpFlow(\n",
        "                                    prev_frame=generation.Artifact(type=generation.ARTIFACT_IMAGE, binary=image_to_jpg_bytes(video_prev_frame)),\n",
        "                                    next_frame=generation.Artifact(type=generation.ARTIFACT_IMAGE, binary=image_to_jpg_bytes(video_next_frame)),\n",
        "                                )\n",
        "                            ))\n",
        "                        video_prev_frame = video_next_frame\n",
        "                        color_match_image = video_next_frame\n",
        "                if len(ops):\n",
        "                    prior_frames, mask = image_xform(stub, prior_frames, ops, TRANSFORM_ENGINE_ID)\n",
        "                    inpaint_mask = mask if args.inpaint_border else None\n",
        "\n",
        "                    depth_map = prior_frames.pop(0) if len(prior_frames) == 3 else None\n",
        "                    if depth_map is not None and args.save_depth_maps:\n",
        "                        cv2.imwrite(os.path.join(out_dir, f\"depth_{frame_idx:05d}.png\"), depth_map)\n",
        "\n",
        "                    if inpaint_mask is not None:\n",
        "                        for i in range(len(prior_frames)):\n",
        "                            prior_frames[i] = image_inpaint(stub, prior_frames[i], inpaint_mask, prompts, weights, steps//2, seed, args.cfg_scale)\n",
        "                        inpaint_mask = None\n",
        "            \"\"\"\n",
        "            if len(prior_frames):\n",
        "                ops, prior_frames, mask, inpaint_mask, color_match_image = self.build_prior_frame_transforms(prior_frames, frame_idx, args, prompts, weights, steps, seed, color_match_image)\n",
        "            \n",
        "\n",
        "            # either run diffusion or emit an inbetween frame\n",
        "            if (frame_idx-diffusion_cadence_ofs) % diffusion_cadence == 0:\n",
        "                if inpaint_mask is not None:\n",
        "                    prior_frames[-1] = image_inpaint(stub, prior_frames[-1], inpaint_mask, prompts, weights, steps//2, seed, args.cfg_scale)\n",
        "                    inpaint_mask = None\n",
        "                strength = self.frame_args.strength_series[frame_idx]\n",
        "\n",
        "                # apply additional noising and color matching to previous frame to use as init\n",
        "                init_image = prior_frames[-1] if len(prior_frames) and strength > 0 else None\n",
        "                if init_image is not None:\n",
        "                    noise = self.frame_args.noise_series[frame_idx]\n",
        "                    brightness = self.frame_args.brightness_series[frame_idx]\n",
        "                    contrast = self.frame_args.contrast_series[frame_idx]\n",
        "                    mix_in = self.frame_args.video_mix_in_series[frame_idx]\n",
        "                    ops = []\n",
        "                    if args.color_coherence != 'None' and color_match_image is not None:                    \n",
        "                        ops.append(generation.TransformOperation(color_match=generation.TransformColorMatch(\n",
        "                            color_mode=color_match_from_string(args.color_coherence),\n",
        "                            image=generation.Artifact(type=generation.ARTIFACT_IMAGE, binary=image_to_jpg_bytes(color_match_image))\n",
        "                        )))\n",
        "                    if mix_in > 0 and video_prev_frame is not None:\n",
        "                        ops.append(generation.TransformOperation(blend=generation.TransformBlend(\n",
        "                            amount=mix_in, \n",
        "                            target=generation.Artifact(type=generation.ARTIFACT_IMAGE, binary=image_to_jpg_bytes(video_prev_frame))\n",
        "                        )))\n",
        "                    if brightness != 1.0 or contrast != 1.0:\n",
        "                        ops.append(generation.TransformOperation(contrast=generation.TransformContrast(\n",
        "                            brightness=brightness, contrast=contrast\n",
        "                        )))\n",
        "                    if noise > 0:\n",
        "                        ops.append(generation.TransformOperation(add_noise=generation.TransformAddNoise(amount=noise, seed=seed)))\n",
        "                    if len(ops):\n",
        "                        init_image = image_xform(stub, [init_image], ops, TRANSFORM_ENGINE_ID)[0][0]\n",
        "\n",
        "                # generate the next frame\n",
        "                sampler = sampler_from_string(args.sampler.lower())\n",
        "                guidance = guidance_from_string(args.clip_guidance)\n",
        "                noise_scale = self.frame_args.noise_scale_series[frame_idx]\n",
        "                image = image_gen(\n",
        "                    stub, \n",
        "                    args.W, args.H, \n",
        "                    prompts, weights, \n",
        "                    steps, seed, args.cfg_scale, sampler, \n",
        "                    init_image, strength,\n",
        "                    init_noise_scale=noise_scale, \n",
        "                    guidance_preset=guidance\n",
        "                )\n",
        "\n",
        "                if color_match_image is None:\n",
        "                    color_match_image = image\n",
        "                if not len(prior_frames):\n",
        "                    prior_frames = [image, image]\n",
        "                \n",
        "                cv2.imwrite(os.path.join(out_dir, f'frame_{frame_idx:05}.png'), prior_frames[1])\n",
        "                display_frame(prior_frames[1])\n",
        "                prior_frames[0] = prior_frames[1]\n",
        "                prior_frames[1] = image\n",
        "                diffusion_cadence_ofs = frame_idx\n",
        "            else:\n",
        "                # smoothly blend between prior frames\n",
        "                tween = ((frame_idx-diffusion_cadence_ofs) % diffusion_cadence) / float(diffusion_cadence)\n",
        "                t = image_mix(prior_frames[0], prior_frames[1], tween)\n",
        "                cv2.imwrite(os.path.join(out_dir, f'frame_{frame_idx:05}.png'), t)\n",
        "                display_frame(t)\n",
        "\n",
        "            if not args.locked_seed:\n",
        "                seed += 1\n",
        "\n",
        "# create folder for frames output\n",
        "timestring = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "out_dir = os.path.join(outputs_path, timestring)\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "print(f\"Saving animation frames to {out_dir}...\")\n",
        "\n",
        "args = SimpleNamespace(**Args())\n",
        "artist = Animator(args, out_dir)\n",
        "artist.render_animation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "cellView": "form",
        "id": "aWhJnLNX3prL"
      },
      "outputs": [],
      "source": [
        "#@title Create video from frames\n",
        "skip_video_for_run_all = False #@param {type: 'boolean'}\n",
        "fps = 12 #@param {type:\"number\"}\n",
        "\n",
        "if skip_video_for_run_all == True:\n",
        "    print('Skipping video creation, uncheck skip_video_for_run_all if you want to run it')\n",
        "else:\n",
        "    image_path = os.path.join(out_dir, \"frame_%05d.png\")\n",
        "    mp4_path = os.path.join(out_dir, f\"{timestring}.mp4\")\n",
        "\n",
        "    print(f\"Compiling animation frames to {mp4_path}...\")\n",
        "\n",
        "    cmd = [\n",
        "        'ffmpeg',\n",
        "        '-y',\n",
        "        '-vcodec', 'png',\n",
        "        '-r', str(fps),\n",
        "        '-start_number', str(0),\n",
        "        '-i', image_path,\n",
        "        '-c:v', 'libx264',\n",
        "        '-vf',\n",
        "        f'fps={fps}',\n",
        "        '-pix_fmt', 'yuv420p',\n",
        "        '-crf', '17',\n",
        "        '-preset', 'veryfast',\n",
        "        mp4_path\n",
        "    ]\n",
        "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    stdout, stderr = process.communicate()\n",
        "    if process.returncode != 0:\n",
        "        print(stderr)\n",
        "        raise RuntimeError(stderr)\n",
        "\n",
        "    mp4 = open(mp4_path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display.display( display.HTML(f'<video controls loop><source src=\"{data_url}\" type=\"video/mp4\"></video>') )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
